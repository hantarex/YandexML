{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_tpu10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hantarex/YandexML/blob/master/test_tpu10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tGUGi07FuiE",
        "colab_type": "code",
        "outputId": "47c0a123-9a82-4bc2-a5a5-3d143940718a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5Wl3YShCUEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/gdrive/My Drive/MachineLearning/Yandex/GTSRB_Final_Training_Images.zip' -d './'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5STgQgLCT6M",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEyK_fhecUoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import skimage\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import imgaug.augmenters as iaa\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUAdAp3LcuU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if tf.__version__.split('.')[1] == '13':\n",
        "#     raise Exception('This notebook is not compatible with Tensorflow 1.13, please use the previous version at https://github.com/tensorflow/tpu/blob/913cf31d85bc31541fbdafa9d0b87db71dd6dcba/tools/colab/shakespeare_with_tpu_and_keras.ipynb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXL-luYPdOHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ia8W6yNdRV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_kernel_sizes = [\n",
        "    [3, 3],\n",
        "    [3, 3],\n",
        "    [3, 2, 1],\n",
        "    [3, 2, 1],\n",
        "]\n",
        "conv_n_maps = [\n",
        "    [16, 16],\n",
        "    [32, 32],\n",
        "    [64, 64, 64],\n",
        "    [128, 128, 128],\n",
        "]\n",
        "conv_n_padding = [\n",
        "    [\"SAME\", \"SAME\"],\n",
        "    [\"SAME\", \"SAME\"],\n",
        "    [\"SAME\", \"SAME\", \"SAME\"],\n",
        "    [\"SAME\", \"SAME\", \"SAME\"],\n",
        "]\n",
        "conv_max_pool = [\n",
        "    (2, 2),\n",
        "    (2, 2),\n",
        "    (2, 2),\n",
        "    (2, 2),\n",
        "]\n",
        "last_layer = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKf-4XrBdnGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_gen = 20\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PpDTr2_dwEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE8vF1UkeQF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "google_path = \"/gdrive/My Drive/MachineLearning/Yandex/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea9o_D0qhWtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main_path = \"./\"\n",
        "data_directory = main_path + \"GTSRB/Final_Training/Images/\"\n",
        "test_directory = main_path + \"GTSRB/Final_Test/Images/\"\n",
        "save_weight_dir = google_path + \"models\" + \"/\" + \"colab15\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGX3fNv9gMR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_files():\n",
        "  exist = False\n",
        "  for d in os.listdir(main_path):\n",
        "    if d.endswith(\".tfrecord\"):\n",
        "      exist = True\n",
        "      break\n",
        "  return exist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adf2WzHPB5u-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "directories = [d for d in os.listdir(data_directory)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoDeoWdnB5eo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = pd.DataFrame()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63vvK_ZbEBcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for d in directories:\n",
        "    label_directory = os.path.join(data_directory, d)\n",
        "    file_names = [os.path.join(label_directory, f) for f in os.listdir(label_directory) if f.endswith(\".ppm\")]\n",
        "    labels_names = [os.path.join(label_directory, f) for f in os.listdir(label_directory) if f.endswith(\".csv\")]\n",
        "    for label_file in labels_names:\n",
        "        lb = pd.read_csv(label_file, sep=';')\n",
        "        lb['Filename'] = lb['Filename'].map(lambda x: os.path.join(label_directory, x))\n",
        "        labels = labels.append(lb, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZpK4H3GEF37",
        "colab_type": "code",
        "outputId": "1865e03b-da1d-414e-908b-08f0bda2a245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "labels.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 39209 entries, 0 to 39208\n",
            "Data columns (total 8 columns):\n",
            "Filename    39209 non-null object\n",
            "Width       39209 non-null int64\n",
            "Height      39209 non-null int64\n",
            "Roi.X1      39209 non-null int64\n",
            "Roi.Y1      39209 non-null int64\n",
            "Roi.X2      39209 non-null int64\n",
            "Roi.Y2      39209 non-null int64\n",
            "ClassId     39209 non-null int64\n",
            "dtypes: int64(7), object(1)\n",
            "memory usage: 2.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg9zlRbpEKPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "width = int(round(labels['Width'].mean()))\n",
        "height = int(round(labels['Height'].mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUBIIIXIEddP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GenerateTFRecord:\n",
        "    def __init__(self, data, batch_img=5):\n",
        "        self.data = data\n",
        "        self.batch_img = batch_img\n",
        "        self.seq = seq = iaa.Sequential([\n",
        "            iaa.GaussianBlur((0, 3.0), name=\"GaussianBlur\"),\n",
        "            iaa.Dropout(0.02, name=\"Dropout\"),\n",
        "            iaa.Affine(translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}),\n",
        "            iaa.Affine(rotate=(-10, 10)),\n",
        "            iaa.Multiply((0.1, 1.2))\n",
        "        ])\n",
        "\n",
        "    def convert_image_folder(self, tfrecord_file_name):\n",
        "        # Get all file names of images present in folder\n",
        "        options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n",
        "        i=1\n",
        "        file_count=0\n",
        "        writer = tf.python_io.TFRecordWriter(tfrecord_file_name + \"_\" + str(file_count) + \".tfrecord\", options=options)\n",
        "        for _, x in self.data.iterrows():\n",
        "            examples = self._convert_image(x)\n",
        "            if i%(len(self.data)//10) == 0:\n",
        "                file_count+=1\n",
        "                file_name = tfrecord_file_name + \"_\" + str(self.batch_img) + \"_\" + str(file_count) + \".tfrecord\";\n",
        "                print(\"Genarating: \" + file_name)\n",
        "                writer.close()\n",
        "                writer = tf.python_io.TFRecordWriter(file_name, options=options)\n",
        "            i+=1\n",
        "            for example in examples:\n",
        "                writer.write(example.SerializeToString())\n",
        "        writer.close()\n",
        "\n",
        "    def _convert_image(self, x):\n",
        "        examples = []\n",
        "        image_data = cv2.imread(x['Filename'])\n",
        "        image_str = image_data.reshape(-1)\n",
        "        img_shape = image_data.shape\n",
        "        examples.append(tf.train.Example(features=tf.train.Features(feature={\n",
        "            'filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[x['Filename'].encode('utf-8')])),\n",
        "            'image': tf.train.Feature(float_list=tf.train.FloatList(value=image_str)),\n",
        "            'shape': tf.train.Feature(int64_list=tf.train.Int64List(value=img_shape)),\n",
        "            'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[x['ClassId']]))\n",
        "        })))\n",
        "        return examples\n",
        "\n",
        "    def _get_label_with_filename(self, filename):\n",
        "        basename = os.path.basename(filename).split('.')[0]\n",
        "        basename = basename.split('_')[0]\n",
        "        return self.labels[basename]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqEo-EMreHpj",
        "colab_type": "code",
        "outputId": "fc2dc946-127e-40c6-b129-5ebd230e368d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "if check_files() is False:\n",
        "  X_train, X_test = train_test_split(\n",
        "      labels, test_size=0.3, random_state=42, stratify=labels['ClassId']\n",
        "  )\n",
        "  GenerateTFRecord(X_train, batch_img=0).convert_image_folder('images_train')\n",
        "  GenerateTFRecord(X_test, batch_img=0).convert_image_folder('images_test')\n",
        "#   files = [google_path + d for d in os.listdir(google_path) if d.endswith(\".tfrecord\") and d.startswith(\"images_\")]\n",
        "#   for file in files:\n",
        "#     shutil.copy(file, main_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Genarating: images_train_0_1.tfrecord\n",
            "Genarating: images_train_0_2.tfrecord\n",
            "Genarating: images_train_0_3.tfrecord\n",
            "Genarating: images_train_0_4.tfrecord\n",
            "Genarating: images_train_0_5.tfrecord\n",
            "Genarating: images_train_0_6.tfrecord\n",
            "Genarating: images_train_0_7.tfrecord\n",
            "Genarating: images_train_0_8.tfrecord\n",
            "Genarating: images_train_0_9.tfrecord\n",
            "Genarating: images_train_0_10.tfrecord\n",
            "Genarating: images_test_0_1.tfrecord\n",
            "Genarating: images_test_0_2.tfrecord\n",
            "Genarating: images_test_0_3.tfrecord\n",
            "Genarating: images_test_0_4.tfrecord\n",
            "Genarating: images_test_0_5.tfrecord\n",
            "Genarating: images_test_0_6.tfrecord\n",
            "Genarating: images_test_0_7.tfrecord\n",
            "Genarating: images_test_0_8.tfrecord\n",
            "Genarating: images_test_0_9.tfrecord\n",
            "Genarating: images_test_0_10.tfrecord\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poUtgwT3e0XP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_generator(name, batch=100, batch_img=5, with_gen=True, aug=None):\n",
        "    def _resize_function(image_decoded, label):\n",
        "        image_decoded.set_shape([None, None, None])\n",
        "        image_resized = tf.image.resize_images(image_decoded, [width, height])\n",
        "        image_resized /= 255.0\n",
        "        return image_resized, label\n",
        "    \n",
        "    def extract_fn(data_record):\n",
        "        features = {\n",
        "            'filename': tf.FixedLenFeature([], tf.string),\n",
        "            'image': tf.VarLenFeature(tf.float32),\n",
        "            'shape': tf.FixedLenFeature([3], tf.int64),\n",
        "            'label': tf.FixedLenFeature([], tf.int64)\n",
        "        }\n",
        "        # Extract the data record\n",
        "        sample = tf.parse_single_example(data_record, features)\n",
        "        shape = sample['shape']\n",
        "        image = tf.sparse.to_dense(sample['image'])\n",
        "        image = tf.reshape(image, shape)\n",
        "        label = sample['label']\n",
        "        return image, label\n",
        "\n",
        "    files = [main_path + d for d in os.listdir(main_path) if d.endswith(\".tfrecord\") and d.startswith(name)]\n",
        "    print(files)\n",
        "    dataset = tf.data.TFRecordDataset(files, compression_type=\"GZIP\")\n",
        "    dataset = dataset.map(extract_fn)\n",
        "    dataset = dataset.map(_resize_function)\n",
        "    if aug is not None:\n",
        "        dataset = dataset.map(lambda x, y: (\n",
        "            tf.py_func(lambda x: aug.augment_images(x), [x], tf.float32),\n",
        "            y\n",
        "        ))\n",
        "    shapes = (tf.TensorShape([width, height, 3]), tf.TensorShape([]))\n",
        "    dataset = dataset.apply(tf.contrib.data.assert_element_shape(shapes))\n",
        "    dataset = dataset.batch(batch)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQhYtrtWfKC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq = iaa.Sequential([\n",
        "    iaa.GaussianBlur((0, 3.0), name=\"GaussianBlur\"),\n",
        "    iaa.Dropout(0.02, name=\"Dropout\"),\n",
        "    iaa.Affine(translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}),\n",
        "    iaa.Affine(rotate=(-10, 10)),\n",
        "    iaa.Multiply((0.05, 1.2))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgXDWxeofMIv",
        "colab_type": "code",
        "outputId": "19d7e25e-50b6-40c7-b5c8-a8e75042b8a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "dataset = create_generator(name=\"images_train_0\", batch_img=batch_gen, batch=batch_size)\n",
        "dataset_test = create_generator(name=\"images_test_0\", batch_img=batch_gen, batch=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./images_train_0_6.tfrecord', './images_train_0.tfrecord', './images_train_0_1.tfrecord', './images_train_0_2.tfrecord', './images_train_0_4.tfrecord', './images_train_0_8.tfrecord', './images_train_0_5.tfrecord', './images_train_0_10.tfrecord', './images_train_0_9.tfrecord', './images_train_0_7.tfrecord', './images_train_0_3.tfrecord']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0617 11:04:18.507228 140324689704832 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['./images_test_0_9.tfrecord', './images_test_0_1.tfrecord', './images_test_0_2.tfrecord', './images_test_0_7.tfrecord', './images_test_0_6.tfrecord', './images_test_0_8.tfrecord', './images_test_0_3.tfrecord', './images_test_0_4.tfrecord', './images_test_0_5.tfrecord', './images_test_0.tfrecord', './images_test_0_10.tfrecord']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J25h5tD_fN86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augment(x, y):\n",
        "    return tf.py_func(lambda x: seq.augment_images(x), [x], tf.float32), y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR1THuLVfcQE",
        "colab_type": "code",
        "outputId": "06400cea-40b4-422d-d668-4862ed92caad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "dataset_aug = [dataset.map(augment) for _ in range(batch_gen)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0617 11:04:18.614043 140324689704832 deprecation.py:323] From <ipython-input-22-bbc1163ffa5c>:2: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jw19WOFfdYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for d in dataset_aug: dataset = dataset.concatenate(d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnP16lPpfe56",
        "colab_type": "code",
        "outputId": "93a5afac-6a64-4e24-b6ad-d13206ad955e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: (<unknown>, (?,)), types: (tf.float32, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohgkav4wfgBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shapes = (tf.TensorShape([None, width, height, None]), tf.TensorShape([None]))\n",
        "dataset = dataset.apply(tf.contrib.data.assert_element_shape(shapes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf51ShwUfhyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset.cache(main_path + \"/cache\").shuffle(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-WZoIvufjrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(name):\n",
        "    inputs = tf.keras.Input(shape=(width, height, 3))\n",
        "    prev_layer = inputs\n",
        "    for num_conv, _ in enumerate(conv_kernel_sizes):\n",
        "        concat = tf.keras.layers.Conv2D(\n",
        "            filters=conv_n_maps[num_conv][-1],\n",
        "            kernel_size=conv_kernel_sizes[num_conv][-1],\n",
        "            padding=conv_n_padding[num_conv][-1],\n",
        "            activation=\"relu\"\n",
        "        )(prev_layer)\n",
        "        # concat = tf.keras.layers.MaxPool2D(conv_max_pool[num_conv])(concat)\n",
        "        for num_map, _ in enumerate(conv_n_maps[num_conv]):\n",
        "            prev_layer = tf.keras.layers.Conv2D(\n",
        "                filters=conv_n_maps[num_conv][num_map],\n",
        "                kernel_size=conv_kernel_sizes[num_conv][num_map],\n",
        "                padding=conv_n_padding[num_conv][num_map],\n",
        "                activation=\"relu\"\n",
        "            )(prev_layer)\n",
        "        prev_layer = tf.keras.layers.Add()([prev_layer, concat])\n",
        "        prev_layer = tf.keras.layers.BatchNormalization()(prev_layer)\n",
        "        prev_layer = tf.keras.layers.MaxPool2D(conv_max_pool[num_conv])(prev_layer)\n",
        "        prev_layer = tf.keras.layers.Dropout(0.25)(prev_layer)\n",
        "\n",
        "    output_m = tf.keras.layers.Flatten()(prev_layer)\n",
        "    pred = tf.keras.layers.Dense(last_layer, kernel_initializer=\"glorot_uniform\", activation=tf.nn.relu)(output_m)\n",
        "    pred = tf.keras.layers.BatchNormalization()(pred)\n",
        "    pred = tf.keras.layers.Dropout(0.5)(pred)\n",
        "    pred = tf.keras.layers.Dense(43, kernel_initializer=\"glorot_uniform\", activation='softmax')(pred)\n",
        "    model_train = tf.keras.Model(inputs=inputs, outputs=pred, name=name)\n",
        "    model_train.compile(\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        metrics=['sparse_categorical_accuracy']\n",
        "    )\n",
        "    model_train.summary()\n",
        "    return model_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5P9cg_rfli-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.keras.backend.clear_session()\n",
        "\n",
        "# resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
        "# tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "# strategy = tf.contrib.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugh25710fpvA",
        "colab_type": "code",
        "outputId": "e4f5629b-61c7-4903-b3cf-6df61140e74f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1516
        }
      },
      "source": [
        "# with strategy.scope():\n",
        "model = model(\"test\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0617 11:04:18.975279 140324689704832 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"test\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 51, 50, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 51, 50, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 51, 50, 16)   2320        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 51, 50, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 51, 50, 16)   0           conv2d_2[0][0]                   \n",
            "                                                                 conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 51, 50, 16)   64          add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 25, 25, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 25, 25, 16)   0           max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 25, 25, 32)   4640        dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 25, 25, 32)   9248        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 25, 25, 32)   4640        dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 25, 25, 32)   0           conv2d_5[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 25, 25, 32)   128         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 12, 12, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 12, 12, 32)   0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 12, 12, 64)   18496       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 12, 12, 64)   16448       conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 12, 12, 64)   4160        conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 12, 12, 64)   2112        dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 12, 12, 64)   0           conv2d_9[0][0]                   \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 12, 12, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 6, 6, 64)     0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 6, 6, 128)    73856       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 6, 6, 128)    65664       conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 6, 6, 128)    16512       conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 6, 6, 128)    8320        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 6, 6, 128)    0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 6, 6, 128)    512         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 3, 3, 128)    0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1152)         0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          295168      flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 256)          1024        dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 256)          0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 43)           11051       dropout_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 535,515\n",
            "Trainable params: 534,523\n",
            "Non-trainable params: 992\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omL-Zm0CiHB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.isfile(save_weight_dir + \".index\"):\n",
        "    print(\"Load weights!\")\n",
        "    model.load_weights(save_weight_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-TQrJPLiJmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStoppingAtMinLoss(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, patience=0):\n",
        "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
        "        self.patience = patience\n",
        "        self.best_weights = None\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.best = np.Inf\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current = logs.get('loss')\n",
        "        self.model.save_weights(save_weight_dir + \"_epoch\")\n",
        "        if np.less(current, self.best):\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            # Record the best weights if current results is better (less).\n",
        "            self.best_weights = self.model.get_weights()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "                print('Restoring model weights from the end of the best epoch.')\n",
        "                self.model.set_weights(self.best_weights)\n",
        "                self.model.save_weights(save_weight_dir)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VrwQYT7fwhh",
        "colab_type": "code",
        "outputId": "622881b0-90da-46b7-bc66-ba1ace27bac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model.fit(\n",
        "    dataset,\n",
        "    epochs=30,\n",
        "    callbacks=[\n",
        "        EarlyStoppingAtMinLoss()\n",
        "    ],\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "5775/5775 [==============================] - 200s 35ms/step - loss: 0.0115 - sparse_categorical_accuracy: 0.9966\n",
            "Epoch 2/30\n",
            "5775/5775 [==============================] - 191s 33ms/step - loss: 0.0108 - sparse_categorical_accuracy: 0.9968\n",
            "Epoch 3/30\n",
            "5773/5775 [============================>.] - ETA: 0s - loss: 0.0108 - sparse_categorical_accuracy: 0.9967Restoring model weights from the end of the best epoch.\n",
            "5775/5775 [==============================] - 192s 33ms/step - loss: 0.0108 - sparse_categorical_accuracy: 0.9967\n",
            "Epoch 00003: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9f7002e518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gqKoMZViPgj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a19aa332-c21e-4219-8d70-9f6d1e5fd914"
      },
      "source": [
        "model.evaluate(dataset_test)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  11763/Unknown - 65s 5ms/step - loss: 0.0058 - sparse_categorical_accuracy: 0.9987"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.005774125717965833, 0.9987248]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3diszrZf7qD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}